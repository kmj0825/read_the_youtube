{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prerequisitions "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "889e18982aa8c11d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install gradio==4.4.1\n",
    "!pip install pytube==15.0.0\n",
    "!pip install whisper\n",
    "!pip install langchain==0.0.344"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c1edb83f9ea9e70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import gradio as gr\n",
    "from pytube import YouTube\n",
    "import whisper\n",
    "import time\n",
    "import pickle\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "openai_api_key = \"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\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e718db4643427402"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download Youtube Videos & Transcribe"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "671d3c1554d41f91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def youtube_text(link):\n",
    "    yt = YouTube(link)\n",
    "    yt.streams.filter(only_audio=True).first().download \\\n",
    "        (output_path=\".\", filename=\"test.mp3\")\n",
    "\n",
    "    start = time.time()\n",
    "    model = whisper.load_model(\"small\")\n",
    "    text = model.transcribe(\"test.mp3\")\n",
    "    end = time.time()\n",
    "\n",
    "    print(text[\"text\"])\n",
    "    print(f\"{end - start:.2f}sec\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len, )\n",
    "\n",
    "    full_docs = text[\"text\"]\n",
    "    docs = [Document(page_content=x) for x in text_splitter.split_text(text[\"text\"])]\n",
    "\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "    with open(\"split_example_small.pkl\", \"wb\") as f:\n",
    "        pickle.dump(split_docs, f)\n",
    "\n",
    "    return split_docs, full_docs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "433d13536b92763b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summarize Youtube Text with ChatGPT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88fe252657b2b1ca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def youtube_sum(split_docs, full_docs, API_KEY):\n",
    "    openai_key = API_KEY\n",
    "    llm = ChatOpenAI(temperature=0.7, openai_api_key=openai_key)\n",
    "\n",
    "    # Map prompt\n",
    "    map_template = \"\"\"The following is a set of documents\n",
    "    {docs}\n",
    "    Based on this list of docs, please identify the main themes\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "    map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "    # Reduce prompt\n",
    "    reduce_template = \"\"\"The following is set of summaries:\n",
    "    {doc_summaries}\n",
    "    You need to output two things from the above Video. \n",
    "    1. Write an executive summary \n",
    "    Read the following documents and write a summary that integrates them to quickly identify the main topics of the Video.\n",
    "    Your summary should. \n",
    "    - Must be written in Korean \n",
    "    - Be a single paragraph\n",
    "    - Be descriptive and detailed so that you can tell at a glance what is being said without having to look at the original Video. \n",
    "    2. Choose your keyword \n",
    "    The keywords have the following conditions \n",
    "    - Must be written in Korean \n",
    "    - Must be a single word \n",
    "    - Must be a noun \n",
    "    - Must be a word that appears in the Video \n",
    "    - Must be a word that is not a stopword \n",
    "    - Must be a word that is not a proper noun \n",
    "    - Must be a word that is not a number \n",
    "    - Must be a word that is not a verb \n",
    "    - Must be a word that is not a pronoun \n",
    "    - Must be a word that is not a preposition \n",
    "    - Must be a word that is not a conjunction \n",
    "    - Must be a word that is not an interjection \n",
    "    - Must be a word that is not an adjective \n",
    "    - Must be a word that is not an adverb \n",
    "    - Must be a word that is not a determiner \n",
    "    - Must be a word that is not a particle \n",
    "    - Must be a word that is not a numeral \n",
    "    - Output only one keyword\n",
    "\n",
    "    Here is an example of the final output\n",
    "    Summary: Document_summary\n",
    "    Keyword: keyword\n",
    "\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "    reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "\n",
    "    reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "    combine_documents_chain = StuffDocumentsChain(\n",
    "        llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    "    )\n",
    "\n",
    "    # Combines and iteravely reduces the mapped documents\n",
    "    reduce_documents_chain = ReduceDocumentsChain(\n",
    "        # This is final chain that is called.\n",
    "        combine_documents_chain=combine_documents_chain,\n",
    "        # If documents exceed context for `StuffDocumentsChain`\n",
    "        collapse_documents_chain=combine_documents_chain,\n",
    "        # The maximum number of tokens to group documents into.\n",
    "        token_max=4000,\n",
    "    )\n",
    "\n",
    "    # 2. Map chain\n",
    "    map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "    # Combining documents by mapping a chain over them, then combining results\n",
    "    map_reduce_chain = MapReduceDocumentsChain(\n",
    "        # Map chain\n",
    "        llm_chain=map_chain,\n",
    "        # Reduce chain\n",
    "        reduce_documents_chain=reduce_documents_chain,\n",
    "        # The variable name in the llm_chain to put the documents in\n",
    "        document_variable_name=\"docs\",\n",
    "        # Return the results of the map steps in the output\n",
    "        return_intermediate_steps=False,\n",
    "    )\n",
    "    # Run\n",
    "    result = map_reduce_chain.run(split_docs)\n",
    "    print(result)\n",
    "    with open(\"result.txt\", \"w\") as f:\n",
    "        f.write(result)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f9ea8ac93e7c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extract keywords from the summary "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53cb004e88473a00"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def text_to_arr(result):\n",
    "    text = result\n",
    "\n",
    "    # Regular expression to find the keyword\n",
    "    match = re.search(r\"Keyword:\\s*(\\w+)\", text)\n",
    "\n",
    "\n",
    "    if match:\n",
    "        keyword = match.group(1)\n",
    "        print(\"Keyword:\", keyword) # The keyword is in the first capturing group\n",
    "    else:\n",
    "        match = re.search(r\"키워드:\\s*(\\w+)\", text)\n",
    "        keyword = match.group(1)  # No keyword found\n",
    "        print(\"Keyword:\", keyword)\n",
    "\n",
    "    return keyword"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39b280df4b134cba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get book information via the Aladdin API"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "541b39e60a7c9e80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def aladin_api(keyword, selected_option):\n",
    "    aladin_key = 'ttbkangmj08250027001'\n",
    "    keyword = keyword\n",
    "    all_data = []\n",
    "    if selected_option == \"사회\":\n",
    "        key = keyword\n",
    "        print(key)\n",
    "        url = f\"http://www.aladin.co.kr/ttb/api/ItemSearch.aspx?ttbkey={aladin_key}&Query={key}&QueryType=Keyword&Cover=Big&MaxResults=5\" \\\n",
    "              \"&start=1&SearchTarget=Book&output=js&Sort=SalesPoint&Version=20131101&CategoryId=90853&outofStockFilter=1\"\n",
    "        response = requests.get(url)\n",
    "        response_json = json.loads(response.text)\n",
    "        all_data.append(response_json)\n",
    "\n",
    "    elif selected_option == \"과학\":\n",
    "        key = keyword\n",
    "        print(key)\n",
    "        url = f\"http://www.aladin.co.kr/ttb/api/ItemSearch.aspx?ttbkey={aladin_key}&Query={key}&QueryType=Keyword&Cover=Big&MaxResults=5\" \\\n",
    "              \"&start=1&SearchTarget=Book&output=js&Sort=SalesPoint&Version=20131101&CategoryId=987&outofStockFilter=1\"\n",
    "        response = requests.get(url)\n",
    "        response_json = json.loads(response.text)\n",
    "        all_data.append(response_json)\n",
    "\n",
    "    elif selected_option == \"소설\":\n",
    "        key = keyword\n",
    "        print(key)\n",
    "        url = f\"http://www.aladin.co.kr/ttb/api/ItemSearch.aspx?ttbkey={aladin_key}&Query={key}&QueryType=Keyword&Cover=Big&MaxResults=5\" \\\n",
    "              \"&start=1&SearchTarget=Book&output=js&Sort=SalesPoint&Version=20131101&CategoryId=1&outofStockFilter=1\"\n",
    "        response = requests.get(url)\n",
    "        response_json = json.loads(response.text)\n",
    "        all_data.append(response_json)\n",
    "\n",
    "    elif selected_option == \"경제경영\":\n",
    "        key = keyword\n",
    "        url = f\"http://www.aladin.co.kr/ttb/api/ItemSearch.aspx?ttbkey={aladin_key}&Query={key}&QueryType=Keyword&Cover=Big&MaxResults=5\" \\\n",
    "              \"&start=1&SearchTarget=Book&output=js&Sort=SalesPoint&Version=20131101&CategoryId=170&outofStockFilter=1\"\n",
    "        response = requests.get(url)\n",
    "        response_json = json.loads(response.text)\n",
    "        all_data.append(response_json)\n",
    "        # request 보내기\n",
    "    all_data = json.dumps(all_data, ensure_ascii=False, indent=4)\n",
    "    with open(\"book.json\", \"wb\") as f:\n",
    "        f.write(all_data.encode(\"utf-8\"))\n",
    "    print(type(all_data))\n",
    "    print(all_data)\n",
    "    return all_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29010f9c74455e9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Organize book information"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49633b20c56badbf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def book_output(book_json):\n",
    "    data = json.loads(book_json)\n",
    "\n",
    "    if len(data[0]['item'][0]) != 0:\n",
    "        title1 = data[0]['item'][0]['title']\n",
    "        book_link1 = data[0]['item'][0]['link']\n",
    "        cover_link1 = data[0]['item'][0]['cover']\n",
    "        response1 = requests.get(cover_link1)\n",
    "        image1 = Image.open(BytesIO(response1.content))\n",
    "    else:\n",
    "        title1 = \"No Data\"\n",
    "        book_link1 = \"No Data\"\n",
    "        image1 = \"No Data\"\n",
    "\n",
    "    if len(data[0]['item'][1]) != 0:\n",
    "        title2 = data[0]['item'][1]['title']\n",
    "        book_link2 = data[0]['item'][1]['link']\n",
    "        cover_link2 = data[0]['item'][1]['cover']\n",
    "        response2 = requests.get(cover_link2)\n",
    "        image2 = Image.open(BytesIO(response2.content))\n",
    "    else:\n",
    "        title2 = \"No Data\"\n",
    "        book_link2 = \"No Data\"\n",
    "        image2 = \"No Data\"\n",
    "\n",
    "    if len(data[0]['item'][2]) != 0:\n",
    "        title3 = data[0]['item'][2]['title']\n",
    "        book_link3 = data[0]['item'][2]['link']\n",
    "        cover_link3 = data[0]['item'][2]['cover']\n",
    "        response3 = requests.get(cover_link3)\n",
    "        image3 = Image.open(BytesIO(response3.content))\n",
    "    else:\n",
    "        title3 = \"No Data\"\n",
    "        book_link3 = \"No Data\"\n",
    "        image3 = \"No Data\"\n",
    "\n",
    "    return title1, image1, title2, image2, title3, image3, book_link1, book_link2, book_link3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f448cdabb3fc5321"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_title(API_KEY, link, selected_option):\n",
    "    docs, split_docs = youtube_text(link)\n",
    "    result = youtube_sum(docs, split_docs, API_KEY)\n",
    "    keywords = text_to_arr(result)\n",
    "    all_data = aladin_api(keywords, selected_option)\n",
    "    title1, image1, title2, image2, title3, image3, link1, link2, link3 = book_output(all_data)\n",
    "    return result, title1, image1, title2, image2, title3, image3, link1, link2, link3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4bfe706731312d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GRADIO Operational Functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "530d8274dfbe9e71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the list of options for the Dropdown\n",
    "options_list = [\"사회\", \"과학\", \"소설\", \"경제경영\"]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"Paste your Youtube Link and get the book recommandation\")\n",
    "    with gr.Column():\n",
    "        with gr.Row():\n",
    "            inp1 = gr.Textbox(label=\"Your OpenAI KEY\")\n",
    "            inp2 = gr.Textbox(label=\"Input Link\")\n",
    "        inp3 = gr.Dropdown(choices=options_list, label=\"Select a category\")\n",
    "        btn = gr.Button(\"Find the book\")\n",
    "\n",
    "    with gr.Column():\n",
    "        out1 = gr.Textbox(label=\"Summary\")\n",
    "        with gr.Row():\n",
    "            out2 = gr.Textbox(label=\"Title1\")\n",
    "            out4 = gr.Textbox(label=\"Title2\")\n",
    "            out6 = gr.Textbox(label=\"Title3\")\n",
    "        with gr.Row():\n",
    "            out3 = gr.Image(label=\"Image1\")\n",
    "            out5 = gr.Image(label=\"Image2\")\n",
    "            out7 = gr.Image(label=\"Image3\")\n",
    "        with gr.Row():\n",
    "            out8 = gr.HTML(label=\"Book Link1\")\n",
    "            out9 = gr.HTML(label=\"Book Link2\")\n",
    "            out10 = gr.HTML(label=\"Book Link3\")\n",
    "    btn.click(fn=get_title, inputs=[inp1,inp2,inp3], outputs=[out1, out2, out3, out4, out5, out6, out7, out8, out9, out10])\n",
    "\n",
    "demo.launch()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fe377d84558d4f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
